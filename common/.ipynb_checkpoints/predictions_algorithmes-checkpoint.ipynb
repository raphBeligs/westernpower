{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "demanding-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet, BayesianRidge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "from tqdm import tqdm\n",
    "import common_functions_by_date as cfbd\n",
    "import datetime\n",
    "from scipy.signal import correlate\n",
    "\n",
    "class DataPreprocesser:\n",
    "    def create_lag_features(df, field,exog_andendog_cols, lags_list=None):\n",
    "   \n",
    "        scaler = StandardScaler()\n",
    "        if lags_list is None:\n",
    "            partial = pd.Series(data=pacf(df[field].values, nlags=48))\n",
    "            lags = list(partial[np.abs(partial) >= 0.2].index)\n",
    "            lags.remove(0)\n",
    "        else:\n",
    "            lags = lags_list\n",
    "        \n",
    "        # avoid to insert the time series itself\n",
    "        features = df[exog_andendog_cols].copy()\n",
    "        for l in lags:\n",
    "            features[f\"lag_{l}\"] = df[field].shift(l)\n",
    "    \n",
    "        features = pd.DataFrame(scaler.fit_transform(features[features.columns]),\n",
    "                            columns=features.columns)\n",
    "        features.index = df.index\n",
    "    \n",
    "        return features, scaler, lags\n",
    "    def scale_features(features):\n",
    "        features_df = features.copy()\n",
    "        scaler = StandardScaler()\n",
    "        features_df = pd.DataFrame(scaler.fit_transform(features_df[features_df.columns]),\n",
    "                            columns=features_df.columns)\n",
    "        features_df.index = features.index\n",
    "        return features_df\n",
    "    def apply_phaze_correction_on_signal(signal, phaze):\n",
    "        if phaze < 0:\n",
    "            phaze_signal = np.concatenate((signal[abs(phaze):], np.ones(abs(phaze))*signal[np.isnan(signal) == False][-1]))\n",
    "            phaze_signal[np.isnan(phaze_signal)] = signal[np.isnan(signal) == False][-1]\n",
    "            phaze_signal[np.isnan(signal)] = np.nan\n",
    "            return phaze_signal\n",
    "        elif phaze > 0:\n",
    "            phaze_signal = np.concatenate((np.ones(abs(phaze))*signal[np.isnan(signal) == False][0], signal[:-abs(phaze)]))\n",
    "            phaze_signal[np.isnan(phaze_signal)] = signal[np.isnan(signal) == False][-1]\n",
    "            phaze_signal[np.isnan(signal)] = np.nan\n",
    "            return phaze_signal\n",
    "        else:\n",
    "            return signal\n",
    "    def force_zero_irradiance_for_smooth_signal(smooth_irr, irr):\n",
    "        if irr == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return smooth_irr\n",
    "    def smooth_preprocess_signal(df, signal_field, smooth_type = 'mean', smooth_k=5):\n",
    "        signals_df = df.copy()\n",
    "        smooth_field = 'smooth_{}_{}'.format(smooth_type, signal_field)\n",
    "        if smooth_type == 'mean':\n",
    "            signals_df[smooth_field] = DataPreprocesser.moving_mean_avarage_smoothing(signals_df[signal_field].values, smooth_k)\n",
    "        smooth_phaze = DataPreprocesser.compute_phaze_shift(signals_df[signals_df[signal_field].notna()][signal_field].values, \n",
    "                                       signals_df[signals_df[signal_field].notna()][smooth_field].values)\n",
    "        print('smooth_phaze :', smooth_phaze)\n",
    "        signals_df[smooth_field] = DataPreprocesser.apply_phaze_correction_on_signal(signals_df[smooth_field].values, smooth_phaze)\n",
    "        if signal_field == 'irradiance_Wm-2':\n",
    "            signals_df[smooth_field] = signals_df.apply(lambda x: DataPreprocesser.force_zero_irradiance_for_smooth_signal(\n",
    "                x[smooth_field], x[signal_field]), axis=1)\n",
    "        return signals_df\n",
    "    def ajust_phaze_shift_of_weather_columns(df, field_ref, field_to_phaze, weather_cols):\n",
    "        signals_df = df.copy()\n",
    "        dephasage = DataPreprocesser.compute_phaze_shift(signals_df[signals_df[field_ref].notna()][field_ref].values, \n",
    "                                    signals_df[signals_df[field_ref].notna()][field_to_phaze].values)\n",
    "        print('weather dephasage :', dephasage)\n",
    "        for col_name in weather_cols:\n",
    "            signals_df[col_name] = DataPreprocesser.apply_phaze_correction_on_signal(signals_df[col_name].values, dephasage)\n",
    "        return signals_df\n",
    "    def moving_mean_avarage_smoothing(X,k):\n",
    "        S = np.zeros(X.shape[0])\n",
    "        for t in range(X.shape[0]):\n",
    "            if t < k:\n",
    "                S[t] = np.mean(X[:t+1])\n",
    "            else:\n",
    "                S[t] = np.sum(X[t-k:t])/k\n",
    "        return S\n",
    "    \n",
    "    def compute_phaze_shift(x1, x2):\n",
    "        xcorr = correlate(x1,x2)\n",
    "        nsamples = x1.size\n",
    "        dt = np.arange(1-nsamples, nsamples)\n",
    "        return dt[xcorr.argmax()]\n",
    "    def get_square_irradiance(df, columns):\n",
    "        square_df = df.copy()\n",
    "        for column in columns:\n",
    "            square_df['square_{}'.format(column)] = square_df[column]**2\n",
    "        return square_df\n",
    "    def get_solar_temp_product_columns(df, locations):\n",
    "        product_df = df.copy()\n",
    "        for i in locations:\n",
    "            product_df['solar_temp_location{}'.format(str(i))] = product_df['solar_location{}'.format(str(i))] * product_df['temp_location{}'.format(str(i))]\n",
    "        return product_df\n",
    "    def build_lags_periods_columns_names(lags_period_list):\n",
    "        return ['lag_{}'.format(str(i)) for i in (np.array(lags_period_list)*48*7).tolist()], [i*48*7 for i in lags_period_list]\n",
    "    def get_lags_median_and_mean_train_df(df, field, lags_list, lags_cols, first_day_pred):\n",
    "        df_with_lags = df.loc[df.index.date < first_day_pred,[field]].copy()\n",
    "        for i in range(len(lags_list)):\n",
    "            df_with_lags[lags_cols[i]] = df_with_lags[field].shift(lags_list[i])\n",
    "        df_with_lags['median_{}'.format(field)] = df_with_lags[lags_cols].median(axis=1).values\n",
    "        df_with_lags['mean_{}'.format(field)] = df_with_lags[lags_cols].mean(axis=1).values\n",
    "        return df_with_lags\n",
    "    def get_lags_median_and_mean_predicted_df(df_with_lags, lags_cols, first_day_pred, field):\n",
    "        lags_cols_for_prediction = [field] + lags_cols[:-1]\n",
    "        df_with_lags_pred = df_with_lags.loc[(df_with_lags.index.date >= first_day_pred+datetime.timedelta(days=-7)) & \n",
    "                                             (df_with_lags.index.date < first_day_pred), lags_cols_for_prediction].copy()\n",
    "        df_with_lags_pred.index = df_with_lags_pred.index + pd.DateOffset(days=7)\n",
    "        dict_col = {}\n",
    "        for i in range(len(lags_cols)):\n",
    "            dict_col[lags_cols_for_prediction[i]] = lags_cols[i]\n",
    "        df_with_lags_pred = df_with_lags_pred.rename(columns=dict_col)\n",
    "        df_with_lags_pred['median_{}'.format(field)] = df_with_lags_pred[lags_cols].median(axis=1).values\n",
    "        df_with_lags_pred['mean_{}'.format(field)] = df_with_lags_pred[lags_cols].mean(axis=1).values\n",
    "        return df_with_lags_pred\n",
    "    \n",
    "    def get_lags_and_median_and_mean_predict_and_train_df(df, field,first_day_pred,lags_period_list=[1,2,3]):\n",
    "        lags_names_cols, lags_list = DataPreprocesser.build_lags_periods_columns_names(lags_period_list)\n",
    "        df_with_lags = DataPreprocesser.get_lags_median_and_mean_train_df(df, field, lags_list, lags_names_cols, first_day_pred)\n",
    "        df_with_lags_pred = DataPreprocesser.get_lags_median_and_mean_predicted_df(df_with_lags, lags_names_cols, first_day_pred,field)\n",
    "        all_df_with_lags = pd.concat([df_with_lags, df_with_lags_pred])\n",
    "        return all_df_with_lags\n",
    "\n",
    "class Forecaster:\n",
    "    \n",
    "    def rectify_recursive_forecast(self,y, model_recursive, lags, exog_and_endog_features, \n",
    "                       n_steps=48*7, step=\"30T\",model_rectify_type = 'KNN', only_pred=True):\n",
    "    \n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: pd.Series holding the input time-series to forecast\n",
    "        model: pre-trained machine learning model\n",
    "        lags: list of lags used for training the model\n",
    "        n_steps: number of time periods in the forecasting horizon\n",
    "        step: forecasting time period\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        fcast_values: pd.Series with forecasted values \n",
    "        \"\"\"\n",
    "        \n",
    "        def get_best_knn(X,y):\n",
    "            param_grid = {'n_neighbors': np.arange(20,60).tolist()}\n",
    "            search = GridSearchCV(KNeighborsRegressor(), param_grid, cv=5)\n",
    "            search.fit(X,y)\n",
    "            print(search.best_estimator_)\n",
    "            return search.best_estimator_\n",
    "        def predict_knn(X_train,y_train, X_to_pred, best_rectify_estimator):\n",
    "            if best_rectify_estimator is None:\n",
    "                best_rec_estimator = get_best_knn(X_train,y_train)\n",
    "                best_model = best_rec_estimator\n",
    "                best_model.fit(X_train, y_train)\n",
    "                return best_model.predict(X_to_pred), best_rec_estimator\n",
    "            else:\n",
    "                best_model = best_rectify_estimator\n",
    "                best_model.fit(X_train, y_train)\n",
    "                return best_model.predict(X_to_pred), None\n",
    "    \n",
    "        def predict_next_value(y_df, exog_and_endog_df, lags_list, datetime_pred, model, best_rectify_estimator):\n",
    "            df = exog_and_endog_df.copy()\n",
    "            y_rec = y_df.copy()\n",
    "            y_rec.loc[datetime_pred, :] = 0\n",
    "            df = pd.merge(y_rec, df, how='inner',left_index=True, right_index=True)\n",
    "            features, _, _ = DataPreprocesser.create_lag_features(df, y_df.columns[0],exog_and_endog_df.columns, lags_list)\n",
    "            prediction_rec = model.predict(features[features.index == datetime_pred])\n",
    "            y_rec_rec = y_df.copy()\n",
    "            y_rec_rec.loc[datetime_pred, :] = prediction_rec\n",
    "            exog_and_endog_rec_rec = exog_and_endog_df.copy()\n",
    "            X_train_rec_rec = pd.merge(y_rec_rec, exog_and_endog_rec_rec, how='inner',left_index=True, right_index=True)\n",
    "            features_rec_rec, _, _ = DataPreprocesser.create_lag_features(X_train_rec_rec, y_df.columns[0],exog_and_endog_df.columns, lags_list + [0])\n",
    "            features_rec_rec = features_rec_rec.dropna(how='any')\n",
    "            columns_rec_rec = features_rec_rec.columns\n",
    "            residuals = y_df.copy()\n",
    "            rediduals = residuals[residuals.index >= features_rec_rec.index[0]][y_df.columns[0]].values - model.predict(\n",
    "                features_rec_rec[features.columns][:-1].values)\n",
    "            if model_rectify_type == 'KNN':\n",
    "                residual_predict, best_rec_estimator = predict_knn(features_rec_rec[:-1],rediduals, \n",
    "                                                               features_rec_rec.values[-1:], best_rectify_estimator=best_rectify_estimator)\n",
    "            if model_rectify_type == 'Ridge':\n",
    "                residual_predict = predict_ridge(features_rec_rec[:-1],rediduals, features_rec_rec.values[-1:])\n",
    "            prediction_rec_rec = prediction_rec + residual_predict\n",
    "            y_df.loc[datetime_pred, :] = prediction_rec_rec\n",
    "            return (y_df, best_rec_estimator)\n",
    "    \n",
    "        # get the dates to forecast\n",
    "        pred_datetime = y.index[-1] + pd.Timedelta(minutes=30)\n",
    "        fcast_range = pd.date_range(pred_datetime, \n",
    "                                periods=n_steps, \n",
    "                                freq=step)\n",
    "        fcasted_values = []\n",
    "        target = y.copy()\n",
    "        best_rectify_estimator = None\n",
    "        with tqdm(total=len(fcast_range), file=sys.stdout) as pbar:\n",
    "            for datetime in fcast_range:\n",
    "                if datetime == pred_datetime:\n",
    "                    print(datetime)\n",
    "                    target, best_rec_estimator = predict_next_value(target, exog_and_endog_features, \n",
    "                                                                    lags, datetime, model_recursive, best_rectify_estimator=None)\n",
    "                    best_rectify_estimator = best_rec_estimator\n",
    "                else:\n",
    "                    target, _ = predict_next_value(target, exog_and_endog_features, \n",
    "                                                                    lags, datetime, model_recursive, best_rectify_estimator=best_rectify_estimator)\n",
    "            \n",
    "                pbar.update()\n",
    "        if only_pred:\n",
    "            return target[target.index >= pred_datetime]\n",
    "        else:\n",
    "            return target\n",
    "        \n",
    "    def recursive_rectify_weeks_before(self, df, field,first_day_pred, endog_exog_df, lags, predictions_path, recursive_ml_model, \n",
    "                                       nb_days_for_train=365, weeks_before_nb=4):\n",
    "        for i in range(weeks_before_nb+1):\n",
    "            pred_date = first_day_pred + datetime.timedelta(days=-i*7)\n",
    "            print('week prediction with start day : ', pred_date)\n",
    "            lags_endog_and_exog, _, lags= DataPreprocesser.create_lag_features(df[(df.index.date < pred_date) & \n",
    "                                                             (df.index.date >= pred_date+datetime.timedelta(days=-nb_days_for_train))], \n",
    "                                                          field,endog_exog_df.columns.tolist(), lags)\n",
    "            X_train_pred = lags_endog_and_exog[(max(lags)+1):].values\n",
    "            y_train_pred = df.loc[(df.index.date < pred_date) & (df.index.date >= pred_date+datetime.timedelta(days=-nb_days_for_train)), \n",
    "                                     field][(max(lags)+1):].values\n",
    "            model = recursive_ml_model\n",
    "            model.fit(X_train_pred, y_train_pred)\n",
    "        \n",
    "            rrf = self.rectify_recursive_forecast(df.loc[(df.index.date < pred_date) & (df.index.date >= pred_date+datetime.timedelta(days=-nb_days_for_train)), \n",
    "                                                [field]], model, lags,endog_exog_df)\n",
    "            directory = os.path.join(predictions_path, field)\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            rrf.to_csv(os.path.join(directory, 'rectify_{}_{}.csv'.format(field, cfbd.BatteryPowerDispatcher.format_dispatch_columns(pred_date))))\n",
    "        return \n",
    "    \n",
    "    def read_and_concat_preds_in_directory(pred_dir, nb_weeks_before, first_day_pred, field):\n",
    "        demand_preds=[]\n",
    "        bpd = cfbd.BatteryPowerDispatcher\n",
    "        for i in range(5):\n",
    "            date_pred = first_day_pred + datetime.timedelta(days=-(nb_weeks_before-i)*7)\n",
    "            demand_preds.append(pd.read_csv(os.path.join(pred_dir,'rectify_{}_{}.csv'.format(field, bpd.format_dispatch_columns(date_pred)))\n",
    "                                           ,parse_dates=['datetime'],index_col=['datetime']))\n",
    "        demand_preds = pd.concat(demand_preds)\n",
    "        demand_preds = demand_preds.rename(columns={field: 'pred_{}'.format(field)})\n",
    "        return demand_preds\n",
    "    \n",
    "    def apply_ml_on_predictions_weeks_before(df, preds_dir, first_day_pred, endog_exog_df, field, nb_weeks_before=4, \n",
    "                                          ml_model_for_demand = RandomForestRegressor(random_state=2019, n_estimators = 450)):\n",
    "        demand_preds = Forecaster.read_and_concat_preds_in_directory(preds_dir, nb_weeks_before, first_day_pred, field)\n",
    "        data_weeks_before = pd.merge(demand_preds, endog_exog_df, how='inner', right_index=True, left_index=True)\n",
    "        scaler_pred = StandardScaler()\n",
    "        data_weeks_before_scaled = data_weeks_before.copy()\n",
    "        data_weeks_before_scaled = scaler_pred.fit_transform(data_weeks_before_scaled)\n",
    "        X_train = data_weeks_before_scaled[:-336,:]\n",
    "        y_train = df.loc[(df.index.date >= first_day_pred+datetime.timedelta(days=-7*nb_weeks_before)) & \n",
    "                            (df.index.date < first_day_pred), field].values\n",
    "        model_demand = ml_model_for_demand\n",
    "        model_demand.fit(X_train, y_train)\n",
    "        demand_pred = model_demand.predict(data_weeks_before_scaled[-336:,:])\n",
    "        return demand_pred\n",
    "\n",
    "    \n",
    "class MLPredictor:\n",
    "    @staticmethod\n",
    "    def preprocess_data_for_solar_predictions(train_df, predicted_df, first_day_pred, nb_days_for_train_model=400):\n",
    "        weather_colums = cfbd.DataPreprocesser.get_columns_of_group_names(cfbd.DataPreprocesser,['solar', 'temp'], [1,2,3,4,5,6], df=train_df)\n",
    "        weather_colums.append('sp')\n",
    "        weather_colums.append('POA')\n",
    "        weather_colums.append('GHI')\n",
    "        df_smooth = predicted_df[weather_colums].copy()\n",
    "        df_smooth = pd.concat([train_df[train_df.index.date >= first_day_pred+datetime.timedelta(days=-nb_days_for_train_model)], df_smooth])\n",
    "        df_smooth = DataPreprocesser.smooth_preprocess_signal(df_smooth, 'irradiance_Wm-2')\n",
    "        df_smooth = DataPreprocesser.ajust_phaze_shift_of_weather_columns(df_smooth, 'smooth_mean_irradiance_Wm-2', 'solar_location1', \n",
    "                                                 cfbd.DataPreprocesser.get_columns_of_group_names(cfbd.DataPreprocesser,\n",
    "                                                                                                  ['solar', 'temp'], np.arange(1,7).tolist(),df=train_df))\n",
    "        df_smooth = DataPreprocesser.smooth_preprocess_signal(df_smooth, 'pv_power_mw')\n",
    "        df_smooth = DataPreprocesser.get_square_irradiance(df_smooth, cfbd.DataPreprocesser.get_columns_of_group_names(cfbd.DataPreprocesser,\n",
    "                                                                                                                       ['solar'], [1,2,3,5,6], df=train_df))\n",
    "        df_smooth = DataPreprocesser.get_solar_temp_product_columns(df_smooth, [1,2,3])\n",
    "        return df_smooth\n",
    "    @staticmethod\n",
    "    def predict_pv_power(df, predicted_df, first_day_pred, field_direct = 'smooth_mean_pv_power_mw', columns_for_pv_power_pred=None, \n",
    "                         model_direct = RandomForestRegressor(random_state=2019, n_estimators=400), nb_days_for_train_model=400):\n",
    "        if columns_for_pv_power_pred is None:\n",
    "            columns_pred_power = cfbd.DataPreprocesser.get_columns_of_group_names(cfbd.DataPreprocesser,\n",
    "                                                                                  ['solar'], [1,2,3,5,6], df=df)\n",
    "            for i in [1,2,3]:\n",
    "                columns_pred_power.append('solar_temp_location{}'.format(str(i)))\n",
    "            for i in [1,2,3,5,6]:\n",
    "                columns_pred_power.append('square_solar_location{}'.format(str(i)))\n",
    "            columns_pred_power.append('sp')\n",
    "            columns_pred_power.append('POA')\n",
    "            columns_pred_power.append('GHI')\n",
    "        else:\n",
    "            columns_pred_power = columns_for_pv_power_pred\n",
    "        final_field = 'pv_power_mw'\n",
    "        df_smooth = MLPredictor.preprocess_data_for_solar_predictions(df, predicted_df, first_day_pred, nb_days_for_train_model=nb_days_for_train_model)\n",
    "        power_features = DataPreprocesser.scale_features(df_smooth[columns_pred_power])\n",
    "        X_train = power_features[power_features.index.date < first_day_pred].values\n",
    "        X_test = power_features[power_features.index.date >= first_day_pred].values\n",
    "        y_train = df_smooth.loc[df_smooth.index.date < first_day_pred, [field_direct, final_field]].values\n",
    "        y_train_smooth = y_train[:,0]\n",
    "        y_train_final = y_train[:,1]\n",
    "        model_direct.fit(X_train, y_train_smooth)\n",
    "        y_train_pred = model_direct.predict(X_train)\n",
    "        residuals_train = y_train_final-y_train_pred\n",
    "        y_test_pred = model_direct.predict(X_test)\n",
    "        all_y_pred = np.concatenate([y_train_pred, y_test_pred])\n",
    "        scaler_pred = StandardScaler()\n",
    "        all_y_pred_scaled = scaler_pred.fit_transform(all_y_pred.reshape(all_y_pred.shape[0],1))\n",
    "        y_train_pred_scaled = all_y_pred_scaled[:-(y_test_pred.shape[0])]\n",
    "    \n",
    "        param_grid = {'n_neighbors': np.arange(3,100).tolist()}\n",
    "        model_rectify = KNeighborsRegressor()\n",
    "        search = GridSearchCV(model_rectify, param_grid, cv=5)\n",
    "        search.fit(np.append(X_train,y_train_pred_scaled, axis=1), residuals_train)\n",
    "        best_model_rectify = search.best_estimator_\n",
    "        model_rectify.fit(np.append(X_train,y_train_pred_scaled, axis=1), residuals_train)\n",
    "        y_test_pred_scaled = all_y_pred_scaled[-(y_test_pred.shape[0]):]\n",
    "        y_test_pred_rectify = y_test_pred + model_rectify.predict(\n",
    "            np.append(X_test,y_test_pred_scaled, axis=1))\n",
    "        return y_test_pred_rectify, y_test_pred\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
